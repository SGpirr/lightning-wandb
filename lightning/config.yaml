# lightning.pytorch==2.0.2
fit:
  seed_everything: 4
  trainer:
    accelerator: 'gpu'
    strategy: 'ddp'
    devices: -1
    num_nodes: 1
    precision: '16-mixed'
    logger:
      - class_path: lightning.pytorch.loggers.WandbLogger
        init_args:
          log_model: true
          project: 'MNIST_test'
          save_dir: '../logs'

    callbacks:
      - class_path: lightning.pytorch.callbacks.EarlyStopping
        init_args:
          patience: 5
          monitor: 'val_accuracy'
      - class_path: lightning.pytorch.callbacks.ModelCheckpoint
        init_args:
          monitor: 'val_accuracy'
          mode: 'max'
          save_top_k: 2
          filename: 'mnist-{epoch:02d}-{valid_acc:.5f}'
          # auto_insert_metric_name: true
          
    fast_dev_run: false
    max_epochs: 5
    min_epochs: null
    max_steps: -1
    min_steps: null
    max_time: null
    limit_train_batches: null
    limit_val_batches: null
    limit_test_batches: null
    limit_predict_batches: null
    overfit_batches: 0.0
    val_check_interval: 1.0
    check_val_every_n_epoch: 1
    num_sanity_val_steps: null
    log_every_n_steps: 10
    enable_checkpointing: null
    enable_progress_bar: null
    enable_model_summary: null
    accumulate_grad_batches: 1
    gradient_clip_val: null
    gradient_clip_algorithm: null
    deterministic: null
    benchmark: null
    inference_mode: true
    use_distributed_sampler: true
    profiler: null
    detect_anomaly: false
    barebones: false
    plugins: null
    sync_batchnorm: false
    reload_dataloaders_every_n_epochs: 0
    default_root_dir: null

  ckpt_path: null
  
  data:
    data_dir: '..'
    num_workers: 16
    batch_size: 256
  
  model:
    n_layer_1: 128
    n_layer_2: 256
    lr: 1e-3
    
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 5e-5
      eps: 1e-8
      weight_decay: 0.01
      
  lr_scheduler:
    class_path: torch.optim.lr_scheduler.CosineAnnealingLR
    init_args:
      T_max: 100
      eta_min: 0


